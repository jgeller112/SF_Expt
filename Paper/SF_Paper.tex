\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[pdf]{apa6}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Sans Forgetica is Really Forgettable},
            pdfauthor={Jason Geller, Sara D. Davis, \& Daniel Peterson},
            pdfkeywords={keywords},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\shorttitle{Sans Forgetica}
\affiliation{
\vspace{0.5cm}
\textsuperscript{1} University of Iowa\\\textsuperscript{2} Skidmore College}
\keywords{keywords\newline\indent Word count: X}
\usepackage{csquotes}
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

\usepackage{longtable}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[flushleft]{threeparttable}
\usepackage{threeparttablex}

\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}


\usepackage{lineno}

\linenumbers

\title{Sans Forgetica is Really Forgettable}
\author{Jason Geller\textsuperscript{1}, Sara D. Davis\textsuperscript{2}, \& Daniel Peterson\textsuperscript{2}}
\date{}

\authornote{Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

Enter author note here.

Correspondence concerning this article should be addressed to Jason Geller, Postal address. E-mail: \href{mailto:jason-geller@uiowa.edu}{\nolinkurl{jason-geller@uiowa.edu}}}

\abstract{
Recent claims have demonstrated that Sans Forgetica font serves as a desirbale difficulty--defined as processing difficulty that improves long-term retention. Despite these claims, there is very little empircal evidence. This led us to examine more closely Sans Forgetica as a potential desirable difficulty. In two preregistered experiments, we tested if Sans Forgetica is really unforgetable. In Experiment 1 (\emph{N} = 215), participants studied weakly realted cue-target word pairs with targets presented in either Sans Forgetcia or with missing letters (e.g., G\_RL). Cued recall performance showed a robust generation effect, but no Sans Forgetica memory benefit. In Experiment 2 (\emph{N}=528), participants read a passage on ground water with select sentences presented in either Sans Forgetcia, yellow highlighting, or unchanged. Cued recall for selelct words were better for pre-highlighted information than when no changes to the passage were made. Critically, presenting sentences in Sans Forgetica did not produce better cued recall than pre-highlighted sentences or sentences presented unchanged. Our findings suggests that Sans Forgetica is really forgeticable.


}

\begin{document}
\maketitle

Students want to remember more and forget less. Decades of research have put forth the paradoxical idea that making learning harder (not easier) should have the desirable effect of improving long-term retention of material--called the desirable diffuclty principle (Bjork, 1994). Notable examples of desirable difficulties include having participants generate information from word fragments instead of passively reading intact words (e.g., Slamecka \& Graf, 1978 (NEWER REFERENCE)), spacing out study sessions instead of massing them (e.g., Carpenter, 2017), and having participants engage in retrieval practice after studying instead of simply restudying the information (Kornell \& Vaughn, 2016). Another simple strategy that has gained some attention is to make material more perceptually disfluent. This can be done by changing the material's perceptual characteristics (Diemand-Yaumen, Oppenheimer, \& Vaughan, 2011; French et al., 2013). Visual material that is masked (Mulligan, 1996), inverted (Sungkhasette, Friedman, \& Castel, 2011), presented in an atypical font (Diemand Yaumen et al., 2011), blurred (Rosner, Davis, \& Milliken, 2015), or even in handwritten cursive (Geller, Still, Dark, Carpenter, 2018) have all been shown to produce memory benefits. The desirable effect of perceptual disfluency on memory is called the disfluency effect (Bjork, 2016)

Although appealing as a pedagogical strategy due to the relative ease of implementation, there have been several experiments that failed to find memorial benefits for perceptually disfluent materials (e.g., Magreehan, Serra, Schwartz \& Narciss, 2016; Rhodes \& Castel, 2008, 2009; Rummer, Scheweppe, \& Schewede, 2016; Yue, Castel, \& Bjork, 2013), casting doubt upon the robustness of the disfluency effect. Corrobroating this, A recent meta-analysis by Xie, Zhou, and Liu (2018) with 25 studies and over 3,000 participants found a small, nonsignificant, effect of perceptual disfluency on recall and (\emph{d} = -0.01) and transfer (\emph{d} = 0.03). Despite having no mnnmemonic effect, perceptual \emph{did} produce longer reading times (\emph{d} = 0.52) and produce lower judgments of learning (\emph{d} = -0.043). Experimentally, Geller et al.(2018) and Geller \& Still (2018) manpiulated several boundary conditions (e.g., level of degradation, type of judgement of learning, retentional interval, and testing expectany) and found you can get mnnmeonic benefits from perceptual disflunet mateirals, but it is rather fickle and not at all robust. Taken together, the evidence suggests that utility of perceptual disfluency is rather limited.

Despite the weak evidence, perceptual disfluency is still being touted as a viable learning tool, especially in the popular press. Recently, reputable news soruces like Washington Post (\url{https://www.washingtonpost.com/business/2018/10/05/introducing-sans-forgetica-font-designed-boost-your-memory/}) and NPR (\url{https://www.npr.org/2018/10/06/655121384/sans-forgetica-a-font-to-remember} claimed that a new font called Sans Forgetica can enhance memory. Since the release of those articles, the Sans Forgetica font is available on all operating systems (all you have to do is downlaod the font file), some browsers (e.g., Chrome), and as a phone application. As of this writing no peer-reviewed research or data has been released that supports the assertions of the Sans Forgetica team.

\hypertarget{what-do-we-know-about-sf}{%
\section{What do we know about SF?}\label{what-do-we-know-about-sf}}

There is not a lot information on Sans Forgetica. What we do know is that the typyface itself is a variation of a sans-serif typeface. SF is a typeface that consists of intermitten gaps in letters that are back slanted (see below picture). There are two possible mechains of action that The design features of this typeface require readers of it to \enquote{fill-in} the missing pieces like a puzzle. As it pertains to the empirical validation of the claims made, the website does offer some information about SF and how the original results were obtained, but not enough information to replicate the studies.

Accoring to an interview conducted by Earp (2018), In the first experiment (\emph{N}=96), they had participants read 20 word pairs (e.g., girl - guy) in three new fonts (one of them being SF) and a typical or common font. The font pairs were presented in was counterbalanced participants. What this means is that all fonts were showns, but the same pairs were never presneted in more than one type of font. Each word pair was presnted on the screen for 100 ms (that is super fast\ldots{}). For a final test, they were given the cue (e.g., \emph{girl}) and had to respond with the target (\emph{guy}). What did they find? According to the interview, targets were recalled 68\% of time when presented in a common font. For cue-target pairs in SF, targets were recalled 69\% of the time--a negeliable difference.

In the second experiment ((\emph{N} = 300) participants were presented passages (250 words in total) where one of the paragraphs was presented in SF. Each participant saw five different texts in total. For each text they were asked one question about the part written in SF and another question about the part written in standard Arial. Participants remembered 57\% of the text when a section was written in Sans Forgetica, compared to 50\% of the surrounding text that was written in a plain Arial font.

\hypertarget{current-studies}{%
\section{Current Studies}\label{current-studies}}

The question of whether Sans Forgetica prodices mnnmenomic benefits has clear practical implications. In the educational domian, it would be relatively quick and easy to use Sans Forgetica. However, in order for the Sans Forgetica to be useful, it is importnat to note and understand both its successes and its failures. Using information obtained in Earp (2018) as a jumping off point, we set out to replicate and extend the Sans Forgetica effect in two high-powered preregistered experiments. In two experiments If the mechainsms that may be driving the Sans Forgetica effect.

\hypertarget{experiment-1}{%
\section{Experiment 1}\label{experiment-1}}

In the first study we compared Sans Forgetica against a robust technique known to enhance memory---generation. The generation effect is a phenomenon where information is better remembered when retrieved than if it is simply read. In a prototypical experiment, participants are asked to generate words from word fragments DOLL - DR\_\_ or read intact cue-target pairs (\emph{DOLL-DRESS}). Compared to the intact condition, individuals recall the generated target words at a higher rate. The nature of generation is where the supposed mnnmeoic benefit of SF comes from. We examined this in the current experiment.

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

We recruited 230 people from Amazon's Mechanical Turk Service. Sample size was based on a priori power analyses conducted using PANGEA v0.2 (Westfall, 2016). Sample size was calculated based on the smallest effect of interest (SEOI; Lakens \& Evers, 2014). In this case, we were interested in powering our study to detect a medium-sized effect size (\emph{d} \textgreater{}= .35). We choose this effect size as our SESOI due in part to the small effect sizes seen in actaul classroom studies (Bulter et al., 2014). Therefore, assuming an alpha of .05 and a desired power of 90\%, a sample size of 270 is required to detect whether an effect size of .35 differs from zero. After excluding participants who 1) did not complete every phase of the experiment, 2) started the experiment multiple times, 3) reported experiencing technical problems did not indicate that they were fluent in English {[}\^{}2{]}: This question was not asked during the experiment., or 5) reported seeing our stimuli before, we were left with 115 participants per group.

\hypertarget{materials}{%
\subsection{Materials}\label{materials}}

The preregistration (aspredicted.org) for Experiment 1 can be found here. All materials, data, and analysis scirpts can be found here (\url{https://osf.io/d2vy8/}). The results contained herein are computationally reproducible by going to the primary author's github and clicking on the binder button (\url{https://github.com/jgeller112/SF_Expt1}; \url{https://github.com/jgeller112/SF_Expt2}).

Participants were presented with 22 weakly related cue-target pairs taken from Carpenter et al., 2012){[}\^{}1{]}: Two cue-target pairs () had to be thrown out as they were not preseted due to a coding error. The cue-target pairs were all nouns, 5--7 letters and 1--3 syllables in length, and high in concreteness (400--700) and frequency (at least 30 per million).

\hypertarget{procedure-and-design}{%
\subsection{Procedure and Design}\label{procedure-and-design}}

The experiment began with the presentation of 22 word pairs, shown one at a time, for 2 secconds each. The cue word always appeared on the left and the target always on the right. Immediately proceeding this, participants did a short 2 minute distractor task (anagram generation). Finally participants completed a cued recall test. During cued recall, particpants were presented 24 cues one at a time and asked to provide the target word. Responses were self-paced. Once completed participants clicked on a button to advance to the next question. After they were asked several demographic questions.

We used a 2 x 2 mixed design. The within-subjects factor (Disfluency: fluent vs.~disfluency) was manipulated across items and participants. The between-subjects factor (Difficulty Type: Generation vs.~Sans Forgetcia) was manipulated between participants. For half the participants, targets were presented in sans forgetica while the other half were presented in Arial font; for the other half of participants, targets were presented with missing letters (vowels were replaced by underscores) and the other half were intact (Arial font). After a short 2 minute distractor task (anagram generation), they completed a cued recall test. During cued recall, particpants were presented 24 cues one at a time and asked to provide the target word. After they were thanked and debriefed.

Spell checking was automated with the hunspell package in R (Ooms, 2018) using spellCheck.R. At the next step we manually examined the output to catch incorrect suggestions and to add their own corrections. Becasuse participants were recruited in the United States, we used the American English dictionary. A nice walkthrough on how to use the package can be found in Buchcamam, De Deyne, and Montefinese (2019). Using the package, each response was corrected for misspelings. Corrected spellings are provided in the most probable order, therefore, the first suggestion is selected as the correct answer. Answers were marked correct if they provided the exact response. In order for a response to be judged correctly, the response had to match the correct answer.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{scoring}{%
\subsection{Scoring}\label{scoring}}

Accuracy was automated with the hunspell package in R (Ooms, 2018) using spellCheck.R.A A nice walkthrough on how to use the package can be found in Buchcamam, De Deyne, and Montefinese (2019). Becasuse participants were recruited in the United States, we used the American English dictionary. Eeach participant response was corrected for misspelings. In the package, corrected spellings are provided in the most probable order, therefore, the first suggestion is selected as the correct answer. As a second pass, we went throigh and made sure the program slected the correct spelling. If the response was close to the correct response, it was marked as correct.

\hypertarget{results-1}{%
\section{Results}\label{results-1}}

Accuracy on the cued recall test was examined using a logistical mixed model (logit link) in R (R studio, 2019) using the lme4 package (Bates, Machler, Bolker, and Walker, 2015) with disfluency and diffilcuty type as a fixed effect and random intercepts for subjects (N=233) and target type (N =22) and random slopes for the factor of disfluency by participant and target: full\_model=glmer(acc\textasciitilde{}condition*dis + (1+ dis\textbar{}ResponseID) + (1+dis\textbar{}target), data=sfgen1, contrasts = list(dis=\enquote{contr.sum}, condition=\enquote{contr.sum}), family=\enquote{binomial}, control=glmerControl(optimizer=\enquote{bobyqa},optCtrl=list(maxfun=100000))). This was the most complext model we could get to converge (Barr, Levy, Scheepers, \& Tily, 2013). condition and disflunecy were sum coded (1, -1).

In Experiment 1 there was no effect of difficulty type, \emph{Estimate} = -0.043, \emph{exp(b)} = .961, \emph{SE} .102, \emph{Z} = -.430, \emph{p} = .667, **d* =. There was an effect of disfluency, \emph{Estimate} = 0.224, \emph{exp(b)} = 1.251, \emph{SE} = .062, \emph{Z} = 3.622, \emph{p} \textless{} .001, \emph{d} = .654. Crucially, there was a significant interacion between difficulty type and disfluency, \emph{Estimate} = 0.249, \emph{exp(b)} = 1.28, \emph{SE} = .041, \emph{Z} = 6.098, \emph{p} \textless{} .001, \emph{d} = .67. This reflected a sizeable generation effect, but no SF effect (See figure below). Although not specified in the preregistration, a Bayes factor (BF) using weakly informative default priors for the estimates (Gelamn, Jakulin, Grazia,Pittaum \& Sung Su, 2008) derived from the the full model using brms () and bayestestR indicated more support for a model with the interaction over a model without the interaction (BF = 9.19).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(qualtRics)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(effects)}
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(lme4)}
\KeywordTok{library}\NormalTok{(ggpol)}
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(report)}

\NormalTok{sfgen=}\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"Expt1_data"}\NormalTok{, }\StringTok{"sfgenerate_final.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Missing column names filled in: 'X1' [1]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{full_model=}\KeywordTok{glmer}\NormalTok{(acc}\OperatorTok{~}\NormalTok{condition}\OperatorTok{*}\NormalTok{dis }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{+}\StringTok{ }\NormalTok{dis}\OperatorTok{|}\NormalTok{ResponseID) }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{+}\NormalTok{dis}\OperatorTok{+}\NormalTok{condition}\OperatorTok{|}\NormalTok{target), }\DataTypeTok{data=}\NormalTok{sfgen, }\DataTypeTok{contrasts =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{dis=}\StringTok{"contr.sum"}\NormalTok{, }\DataTypeTok{condition=}\StringTok{"contr.sum"}\NormalTok{), }\DataTypeTok{family=}\StringTok{"binomial"}\NormalTok{, }\DataTypeTok{control=}\KeywordTok{glmerControl}\NormalTok{(}\DataTypeTok{optimizer=}\StringTok{"bobyqa"}\NormalTok{,}\DataTypeTok{optCtrl=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{maxfun=}\DecValTok{100000}\NormalTok{)))}



\KeywordTok{paste}\NormalTok{(}\KeywordTok{report}\NormalTok{(full_model))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "We fitted a logistic mixed model (estimated using ML and BOBYQA optimizer) to predict acc with condition and dis (formula = acc ~ condition * dis). The model included dis, ResponseID, condition and target as random effects (formula = ~1 + dis | ResponseID + ~1 + dis + condition | target). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. Effect sizes were labelled following Chen's (2010) recommendations.The model's total explanatory power is substantial (conditional R2 = 0.60) and the part related to the fixed effects alone (marginal R2) is of 0.01. The model's intercept, corresponding to acc = 0, condition = Generate, dis = disfluent, ResponseID = E10 and target = branch, is at -1.21 (SE = 0.34, 95% CI [-1.88, -0.54], p < .001). Within this model:\n\n  - The effect of condition1 is negative and can be considered as very small and not significant (beta = -0.09, SE = 0.11, 95% CI [-0.30, 0.13], std. beta = -0.09, p = 0.431).\n  - The effect of dis1 is positive and can be considered as very small and significant (beta = 0.21, SE = 0.06, 95% CI [0.09, 0.33], std. beta = 0.22, p < .001).\n  - The effect of condition1:dis1 is positive and can be considered as very small and significant (beta = 0.22, SE = 0.04, 95% CI [0.14, 0.30], std. beta = 0.21, p < .001)."
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ef1 <-}\StringTok{ }\KeywordTok{effect}\NormalTok{(}\StringTok{"condition:dis"}\NormalTok{, full_model) }\CommentTok{#take final glmer model }
\KeywordTok{summary}\NormalTok{(ef1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  condition*dis effect
##                 dis
## condition        disfluent    fluent
##   Generate       0.2952348 0.1507106
##   Sans Forgetica 0.2429061 0.2469149
## 
##  Lower 95 Percent Confidence Limits
##                 dis
## condition        disfluent     fluent
##   Generate       0.1625725 0.08088777
##   Sans Forgetica 0.1343397 0.14399034
## 
##  Upper 95 Percent Confidence Limits
##                 dis
## condition        disfluent    fluent
##   Generate       0.4747772 0.2635241
##   Sans Forgetica 0.3987912 0.3898998
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(ef1)}

\NormalTok{bold <-}\StringTok{ }\KeywordTok{element_text}\NormalTok{(}\DataTypeTok{face =} \StringTok{"bold"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{14}\NormalTok{) }\CommentTok{#axis bold}

\NormalTok{p<-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(x1, }\KeywordTok{aes}\NormalTok{(dis, fit, }\DataTypeTok{fill=}\NormalTok{dis))}\OperatorTok{+}\StringTok{ }\KeywordTok{facet_grid}\NormalTok{(}\OperatorTok{~}\NormalTok{condition)}\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{"identity"}\NormalTok{, }\DataTypeTok{position=}\StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin=}\NormalTok{lower, }\DataTypeTok{ymax=}\NormalTok{upper), }\DataTypeTok{width=}\FloatTok{0.2}\NormalTok{, }\DataTypeTok{position=}\KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width=}\FloatTok{0.9}\NormalTok{),}\DataTypeTok{color=}\StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{(}\DataTypeTok{base_size=}\DecValTok{14}\NormalTok{)}\OperatorTok{+}\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y=}\StringTok{"Proporition Recalled on Final Test"}\NormalTok{, }\DataTypeTok{x=}\StringTok{"Disfluency"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"grey"}\NormalTok{, }\StringTok{"black"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text=}\NormalTok{bold)}

\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\includegraphics{SF_Paper_files/figure-latex/unnamed-chunk-1-1.pdf}

\hypertarget{experiment-2}{%
\section{Experiment 2}\label{experiment-2}}

Experiment 1 failed to find a Sans Forgetica effect. One caveat of Experiment 1 is that simple paired associate learning lacks educational realsim. To remedy this, Experiment 2 tested the effects of SF using more realistic materials. In addition to looking at the effects of Sans Forgetica, we examined the the effects pre-highlighting on memory. Whereas Experiment 1 tested whether Sans Forgetica is driven by generation whereby individuals fill in the missing letters. Experiment 2 examined another possible mechanism of action--that is, the Sans Forgetcia effect might exert its mnnmenonic benefit by making material more distinctive. Specifically, Sans Forgetica may make the marked portion of text more memorable because it stands
out from the surrounding text. Pre-highlighting is purpoted to arise via a similar mechanism. Indeed, some evidence supports this type of role for highlighting: When students read pre-highlighted passages, they recall more of the highlighted information and less of the non-highlighted information compared to students who receive an unmarked copy of the same passage (Fowler and Barker 1974; Silvers and Kreiner 1997). To this end, Experiment 2 compared cued recall performance between Sans Forgetica and with a passage on ground water where some of the material were either presented in: SF, pre-highlighted in yellow, or unmarked. Each condition was manipulated between-subjects.

\hypertarget{participants-1}{%
\subsection{Participants}\label{participants-1}}

Participants were 528 undergraduates who participated for partial completion of course credit. Sample size was based on a priori power analyses conducted using PANGEA v0.2. Sample size was calculated based on the samllest effect of interest (Lakens \& Evers, 2014). In this case, we were interested in powering our study to detect a medium-sized effect size (\emph{d} = .35). Therefore, assuming an alpha of .05 and a desired power of 90\%, a sample size of 170 is required to detect whether an effect size of .35 differs from zero. After excluding participants based on our preregistered exclusion critera, we were left with unequal group sizes. Becasue of this, we ran six more pariticpants per group, giving us 176 participants in each of the three conditions.

\hypertarget{materials-1}{%
\subsection{Materials}\label{materials-1}}

The preregistatiron (aspredicted.org) for Experiment 2 can be found here. All materials, data, and analysis scirpts can be found here (\url{https://osf.io/d2vy8/}). The results contained herein are computationally reproducible by going to the primary author's github and clicking on the binder button (\url{https://github.com/jgeller112/SF_Expt1}; \url{https://github.com/jgeller112/SF_Expt2}).

Participants read a passage on ground water (856 words) taken from from the U.S. Geological Survey (see Yue et al.) Eleven critical phrases\footnote{orginally we had 12 critical phrases but a pilot test showed that one of the questions was repeated twice so we removed one of them and also added a manipulation check question to sure participants were paying attention} each containing a different keyword, were selected from the passage (e.g., the term \emph{recharge} was the keyword in the phrase: Water seeping down from the land surface adds to the ground water and is called recharge water.) and were either presented in SF, highlighted, or unchanged. Then, 11 fill-in-the blank questions were created from these phrases by deleting the keyword and asking participants to provide it on the final test (e.g., Water seeping down from the land surface adds to the ground water and is called \_\_\_\_\_\_\_\_\_\_ water).

\hypertarget{design-and-procedure}{%
\subsection{Design and Procedure}\label{design-and-procedure}}

Participants were randomly assigned to either the pre-highlighted codnition, sans forgetica condition, or normal condition. Our design employed three between-subject variables: pre-highlighting, sans forgetica, and normal.

Participants completed the experiment on-line via the qualtrics survey platform. Participant read the passage on ground water in its entirety. Participants were given 10 minutes to read the passage. Participants in the pre-highlighted condition received some of the passages in yellow highlighting. Participants in the sans forgetcia codnition were presnetd some of the sentences in the sans forgetica font. Participants in the normal passage condition were presented sentences with no changes. All particiapnts were instructed to read the passage as though they were studying material for a class.

After 10 minutes, all participants were given a brief questionnaire (2 questions) asking them to indicate their metacognitive beliefs afte reading the passage. The two questions were: \enquote{Do you feel that the presentation fo the material helped you remember} and \enquote{How likely is it that you will be able to recall material from the passage you just read on a scale of 0 (not likely to recall) to 100 (likely to recall) in 5 minutes?} Participants were then given a short distractor task (anagrams) for 3 minutes. Finally, all participants were given 11 fill-in-the-blank test questions, one at a time. There was 1 manipulation multiple choice questions: What was the passage you read on?."

\hypertarget{results-2}{%
\section{Results}\label{results-2}}

Accuracy on the fill-in-the-blank test was examined using a logistical mixed model (logit link) in R (R studio, 2019) using the lme4 package (Bates, Machler, Bolker, and Walker, 2015) with passage type as a fixed effect and random intercepts for subjects (\emph{n}=528) and questions (\emph{n}=11): acc=glmer(auto\_acc\textasciitilde{}passage\_type+(1\textbar{}ResponseId) + (1\textbar{}Question), data=data, family=\enquote{binomial}). Passage type was treatment coded thus estimates represent simple effects.

We hypothesized that recall for pre-highlighted and sans forgetica sentences would be better remembered than normal sentences and that there would be no recall differences between the highlighted and sans forgetia sentences. Our hypotheses were partially supported (see Figure 2). Results indicated that pre-highlighted sentences were better remembered than sentences presented normally, \emph{Estimate} = .381, \emph{exp(B)} = 1.46, \emph{SE} = .167, \emph{z} = 2.281, \emph{p} = .023 \emph{d} = .81 {[}\^{}3: odds ratios were converted to d by dividing the ln(OR) by 1.81 (Chinn, 2000){]} and were marginally better remmebered than sentences presented in sans forgetcia,\emph{Estimate} = -.317, \emph{exp(B)} = 1.37, \emph{SE} = .168, \emph{z} = -1.89, \emph{p} = .059, \emph{d} = .76. Critically, there was no difference between sentences presented normally and in sans forgetcia, \emph{Estimate} = .065, \emph{exp(B)} = 1.07, \emph{SE} = .167, \emph{z} = 0.386, \emph{p} = 0.700, \emph{d} =.04. A Bayes factor using the brms package (Burkner, 2015) was computed for no difference found that probability of this effect being zero was 12.72 to 1.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(qualtRics)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(afex)}
\KeywordTok{library}\NormalTok{(emmeans)}
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(ggpol)}
\KeywordTok{library}\NormalTok{(knitr)}

\NormalTok{ground <-}\StringTok{ }\NormalTok{qualtRics}\OperatorTok{::}\KeywordTok{read_survey}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"Expt2_Data"}\NormalTok{, }\StringTok{"memory_acc_gw_final.csv"}\NormalTok{))}

\NormalTok{ground_change <-}\StringTok{ }\NormalTok{ground }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Passage=}\KeywordTok{ifelse}\NormalTok{(FL_}\DecValTok{149}\NormalTok{_DO}\OperatorTok{==}\StringTok{"Highlight"}\NormalTok{, }\StringTok{"Pre-highlighted"}\NormalTok{, }\KeywordTok{ifelse}\NormalTok{(FL_}\DecValTok{149}\NormalTok{_DO}\OperatorTok{==}\StringTok{"Passage"}\NormalTok{, }\StringTok{"Sans Forgetica"}\NormalTok{, }\StringTok{"Normal"}\NormalTok{)))}

\CommentTok{#data was collected until the last day of the fall semester 2019 Decemeber13th. }
\CommentTok{# loading needed libraries}
\NormalTok{full_model=}\KeywordTok{glmer}\NormalTok{(auto_acc}\OperatorTok{~}\NormalTok{Passage}\OperatorTok{+}\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{ResponseId) }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{Question), }\DataTypeTok{data=}\NormalTok{ground_change, }\DataTypeTok{family=}\StringTok{"binomial"}\NormalTok{)}
\CommentTok{#fit full model}

\KeywordTok{paste}\NormalTok{(}\KeywordTok{report}\NormalTok{(full_model))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "We fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict auto_acc with Passage (formula = auto_acc ~ Passage). The model included ResponseId and Question as random effects (formula = ~1 | ResponseId + ~1 | Question). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. Effect sizes were labelled following Chen's (2010) recommendations.The model's total explanatory power is substantial (conditional R2 = 0.45) and the part related to the fixed effects alone (marginal R2) is of 0.00. The model's intercept, corresponding to auto_acc = 0, Passage = Normal, ResponseId = R_01xBIjc5ddf5oWd and Question = Q434, is at -0.47 (SE = 0.29, 95% CI [-1.04, 0.10], p = 0.108). Within this model:\n\n  - The effect of PassagePre-highlighted is positive and can be considered as very small and significant (beta = 0.38, SE = 0.17, 95% CI [0.05, 0.71], std. beta = 0.38, p < .05).\n  - The effect of PassageSans Forgetica is positive and can be considered as very small and not significant (beta = 0.06, SE = 0.17, 95% CI [-0.26, 0.39], std. beta = 0.06, p = 0.700)."
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ef1 <-}\StringTok{ }\KeywordTok{effect}\NormalTok{(}\StringTok{"Passage"}\NormalTok{, full_model) }\CommentTok{#take final glmer model }
\KeywordTok{summary}\NormalTok{(ef1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Passage effect
## Passage
##          Normal Pre-highlighted  Sans Forgetica 
##       0.3847685       0.4779490       0.4001575 
## 
##  Lower 95 Percent Confidence Limits
## Passage
##          Normal Pre-highlighted  Sans Forgetica 
##       0.2608280       0.3405702       0.2733269 
## 
##  Upper 95 Percent Confidence Limits
## Passage
##          Normal Pre-highlighted  Sans Forgetica 
##       0.5257163       0.6187467       0.5419488
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(ef1)}

\NormalTok{bold <-}\StringTok{ }\KeywordTok{element_text}\NormalTok{(}\DataTypeTok{face =} \StringTok{"bold"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{14}\NormalTok{) }\CommentTok{#axis bold}
\NormalTok{p<-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(x1, }\KeywordTok{aes}\NormalTok{(Passage, fit, }\DataTypeTok{fill=}\NormalTok{Passage))}\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{"identity"}\NormalTok{, }\DataTypeTok{position=}\StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin=}\NormalTok{lower, }\DataTypeTok{ymax=}\NormalTok{upper), }\DataTypeTok{width=}\FloatTok{0.2}\NormalTok{, }\DataTypeTok{position=}\KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width=}\FloatTok{0.9}\NormalTok{),}\DataTypeTok{color=}\StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{(}\DataTypeTok{base_size=}\DecValTok{14}\NormalTok{)}\OperatorTok{+}\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y=}\StringTok{"Recall Accuracy"}\NormalTok{, }\DataTypeTok{x=}\StringTok{"Passage Type"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_fill_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"grey"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{, }\StringTok{"black"}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text=}\NormalTok{bold, }\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{p }
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{SF_Paper_files/figure-latex/unnamed-chunk-2-1}

\}

\textbackslash{}caption\{Accuracy on Cued Recall Test. Error bars are 95\% CI dervied from the GLMER model.\}\label{fig:unnamed-chunk-2}
\textbackslash{}end\{figure\}

\hypertarget{exploratory-analysis}{%
\section{Exploratory Analysis}\label{exploratory-analysis}}

In Experiment 2 we also asked students about their metacognitive awarness. Specifically we asked them: \enquote{How likely is it that you will be able to recall material from the passage you just read on a scale of 0 (not likely to recall) to 100 (likely to recall) in 5 minutes?} Initials analyses suggest that the normal passage was given higher JOLs (\emph{M} = 57.4, \emph{SE} = 1.97) than the pre-highlighted passage (\emph{M} = 50.3, \emph{SE} = 1.97), t(525) = -7.08, \emph{p} = .023. There were no reliable differences between the pre-highlighted passage and Sans Forgetica (\emph{M} = 53.8, \emph{SE} = 1.97), \emph{t}(525) = -3.52, \emph{p} = .415 or between the passage in Sans Forgetica and the passage presneted normally, \emph{t}(525) = 3.56, \emph{p} = .406.

One potential reason for pre-highlighted information recieving lower JOLs than the normal passage is that pre-highlighted information served to focus participants attention specific parts of the passage. Given the question, pariticpants might thought this would hinder them if tested over the passage as a whole. Future research should

\begin{tabular}{l|r|r|r|r|r}
\hline
contrast & estimate & SE & df & t.ratio & p.value\\
\hline
Highlight - Normal & -7.079546 & 2.7792 & 525 & -2.547332 & 0.0299152\\
\hline
Highlight - Passage & -3.517046 & 2.7792 & 525 & -1.265488 & 0.4153929\\
\hline
Normal - Passage & 3.562500 & 2.7792 & 525 & 1.281844 & 0.4060534\\
\hline
\end{tabular}

\begin{figure}

{\centering \includegraphics{SF_Paper_files/figure-latex/unnamed-chunk-3-1} 

}

\caption{Judgements of learning as a function of passage type.}\label{fig:unnamed-chunk-3}
\end{figure}

\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{\label{tab:unnamed-chunk-3}}
\begin{tabular}{llllll}
\toprule
contrast & \multicolumn{1}{c}{estimate} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{df} & \multicolumn{1}{c}{t.ratio} & \multicolumn{1}{c}{p.value}\\
\midrule
Highlight - Normal & -7.08 & 2.78 & 525.00 & -2.55 & 0.03\\
Highlight - Passage & -3.52 & 2.78 & 525.00 & -1.27 & 0.42\\
Normal - Passage & 3.56 & 2.78 & 525.00 & 1.28 & 0.41\\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{center}
\end{table}

NULL

We hypothezied that sentences pre-highlighted or presented in sans forgetica would be better remembered than sentences presented normally. Further, we predicted that there would be no recall differences between the pre-highligted and the sans forgetica conditions. Our hypothese were only partially confirmed. We found that infromation that was pre-hightlighted had better recall than passages presentened normally, \emph{Estimate} = -.328, \emph{SE} = .166, \emph{z} = -1.97, \emph{p} = .048. Sentences that were pre-highlighted were also remembered marginally better than senetnces presented in sans forgetica,\emph{Estimate} = -.307, \emph{SE} = .167, \emph{z} = -1.84, \emph{p} = .066. Looking at Bayes Factor for this comparison suggests that evidence for a difference between the two conditions is faily weak. Critically, sentences presented in sans forgetcia were not better remembered than sentences presented normally, \emph{Estimate} = -.328, \emph{SE} = .166, \emph{z} = -1.97, \emph{p} = .048, \emph{BF}=).

\hypertarget{dicussion}{%
\section{Dicussion}\label{dicussion}}

Across two experiment
The evidence contained herein suggests that SF does not have the mnemonic effects pruported by its creators. Now it is possible that there is an effect of SF, but the effect size might be smaller than we could detect acorss our two studies. Our SESOI was d = .35. If so, it probably does not have any real educational benefit. It is our conclsuion that SF is really forgetable and you should not be using it as a way to boost leanring.

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}

\endgroup

\end{document}
