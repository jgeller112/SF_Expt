---
title             : "Sans Forgetica is Really Forgettable"
shorttitle        : "Sans Forgetica"

author: 
  - name          : "Jason Geller"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "jason-geller@uiowa.edu"
  - name          : "Sara D. Davis"
    affiliation   : "2"
  - name          : "Daniel Peterson"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "University of Iowa"
  - id            : "2"
    institution   : "Skidmore College"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  
keywords          : "fluency"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "doc"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Students want to remember more and forget less. Decades of research have put forth the paradoxical idea that making learning harder (not easier) should have the desirable effect of improving long-term retention of material--called the desirable diffuclty principle (Bjork, 1994). Notable examples of desirable difficulties include having participants generate information from word fragments instead of passively reading intact words (e.g., Slamecka & Graf, 1978), spacing out study sessions instead of massing them (e.g., Carpenter, 2017), and having participants engage in retrieval practice after studying instead of simply restudying the information (Kornell & Vaughn, 2016). Another simple strategy that has gained some attention is to make material more perceptually disfluent. This can be done by changing the material’s perceptual characteristics (Diemand-Yaumen, Oppenheimer, & Vaughan, 2011; French et al., 2013). Visual material that is masked (Mulligan, 1996), inverted (Sungkhasette, Friedman, & Castel, 2011), presented in an atypical font (Diemand Yaumen et al., 2011), blurred (Rosner, Davis, & Milliken, 2015), or even in handwritten cursive (Geller, Still, Dark, Carpenter, 2018) have all been shown to produce memory benefits. The desirable effect of perceptual disfluency on memory is called the disfluency effect (Bjork, 2016)

Although appealing as a pedagogical strategy, there have been several experiments that failed to find memorial benefits for perceptually disfluent
materials (e.g., Magreehan, Serra, Schwartz & Narciss, 2016; Rhodes & Castel, 2008, 2009; Rummer, Scheweppe, & Schewede, 2016; Yue,
Castel, & Bjork, 2013), casting doubt upon the veracity of the disfluency effect. A recent meta-analysis (),  Recent studies by Geller et al.(2018) and Geller & Still (2018) found that perceptual disfluency can have a beneficial effect on memory, but seems to be rather fickle, thus delimiting its educational usefuleness.   

Given the weak evidence, it came as a surprise to me when a little over a year ago, a font by the name of Sans Forgetica (SF) started getting a ton of press coverage. The mnnmenomic benefits of this font, *based on cognitive psychology*, were being touted in reputable news sources like Washington Post (https://www.washingtonpost.com/business/2018/10/05/introducing-sans-forgetica-font-designed-boost-your-memory/) and NPR (https://www.npr.org/2018/10/06/655121384/sans-forgetica-a-font-to-remember, amongst others. The creators even made the SF font available for mac and pc operating systems--all you have to do is downlaod the font file and you to can remember everything you read :). There is even a Chrome browser extension and cellphone application that allows users to place material in Sans Forgetica. With this much attention and marketing, there has to be solid empirical evidence backing it up, right? Not quite. 


Despite the weak evidence for perceptual disfluency, it came as a surprise to me when a little over a year ago, I saw a font by the name of Sans Forgetica (SF) getting a ton of press coverage. The mnnmenomic benefits of this font, *based on cognitive psychology*, were being touted in reputable news sources like Washington Post (https://www.washingtonpost.com/business/2018/10/05/introducing-sans-forgetica-font-designed-boost-your-memory/) and NPR (https://www.npr.org/2018/10/06/655121384/sans-forgetica-a-font-to-remember, amongst others. The creators even made the SF font available for mac and pc operating systems--all you have to do is downlaod the font file and you to can remember everything you read :). There is even a Chrome browser extension and cellphone application that allows users to place material in Sans Forgetica. With this much attention and marketing, there has to be solid empirical evidence backing it up, right? Not quite. 

# What do we know about SF? 

There is not information about SF. The typyface itself is a variation of a sans-serif typeface. It is a typeface that consists of intermitten gaps in letters that are back slanted (see below picture). The design features of this typeface require readers of it to "fill-in" the missing pieces like a puzzle. As it pertains to the empirical validation of the claims made, the website does offer some information about SF and how the original results were obtained, but not enough information to replicate the studies. 


Earp (2018) conducted an interview with the creators of SF and I was able to glean some details about how SF ws validated. Apparently two studies were conducted. In a lab experiment (*N*=96), they had participants read 20 word pairs (e.g., girl - guy; called a paried associates task in cognitive parlance) in three new fonts (one of them being SF) and a typical or common font. The font pairs were presented in was counterbalanced participants. What this means is that all fonts were showns, but the same pairs were never presneted in more than one type of font. Each word pair was presnted on the screen for 100 ms (that is super fast...). For a final test, they were given the cue (e.g., *girl*) and had to respond with the target (*guy*). What did they find? According to the interview, targets were recalled 68% of time when presented in a common font. For cue-target pairs in SF, targets were recalled 69% of the time--a negeliable difference. 

In an online experiment, participants were presented passages (250 words in total) where one of the paragraphs was presented in SF. Each participant saw five different texts in total. For each text they were asked one question about the part written in SF and another question about the part written in standard Arial. Participants remembered 57% of the text when a section was written in Sans Forgetica, compared to 50% of the surrounding text that was written in a plain Arial font.

At the time of this writing, these studies have not been published nor is there a preprint available. I reached out to the creators of SF, but they refused to share the materials with me. Instead of waiting, I elicited the help of Sara Davis and Daniel Peterson at Skidmore university to test the mnenmomic benefits of Sans Forgetica.

# Experiment 1

In the first study we compared the mnenmonic benefits of SF against a robust technique known to enhance memory—generation. The generation effect is a phenomenon where information is better remembered when retrieved than if it is simply read. In a prototypical experiment,participants are asked to generate words from word fragments DOLL - DR__ or read intact cue-target pairs (*DOLL-DRESS*). Compared to the intact condition, individuals recall the generated target words at a higher rate. The nature of generation is where the supposed mnnmeoic benefit of SF comes from. We examined this in the current experiment. 

## Participants

We recruited 230 people from Amazon’s Mechanical Turk Service. Sample size was calculated based on the smallest effect of interest (SEOI; Lakens & Evers, 2014). In this case, we were interested in powering our study to detect a medium-sized effect size (*d* = .35). We choose this effect size as our SESOI due in part to the small effect sizes seen in actaul classroom studies (Bulter et al., 2014). Therefore, assuming an alpha of .05 and  a desired power of 90%, a sample size of 270 is required to detect whether an effect size of .35 differs from zero. After excluding participants who 1) did not complete every phase of the experiment, 2) started the experiment multiple times, 3) reported experiencing technical problems did not indicate that they were fluent in English [^2]: This question was not asked during the experiment., or 5) reported seeing our stimuli before, we were left with 115 participants per group. 

## Materials

The preregistration (aspredicted.org) for Experiment 1 can be found here. All materials, data, and analysis scirpts can be found here (https://osf.io/d2vy8/). The results contained herein are  computationally reproducible by going to the primary author's github and clicking on the binder button (https://github.com/jgeller112/SF_Expt1; https://github.com/jgeller112/SF_Expt2). 

Participants were presented with 22 weakly related cue-target pairs taken from Carpenter et al., 2012)[^1]: Two cue-target pairs () had to be thrown out as they were not preseted due to a coding error. The cue-target pairs were all nouns, 5–7 letters and 1–3 syllables in length, and high in concreteness (400–700) and frequency (at least 30 per million). 

## Procedure and Design
The experiment began with the presentation of 22 word pairs, shown one at a time, for 2 secconds each. The cue word always appeared on the left and the target always on the right. Immediately proceeding this, participants did a short 2 minute distractor task (anagram generation). Finally participants completed a cued recall test. During cued recall, particpants were presented 24 cues one at a time and asked to provide the target word. Responses were self-paced. Once completed participants clicked on a button to advance to the next question. After they were asked several demographic questions.

We used a 2 x 2 mixed design. The within-subjects factor (Disfluency: fluent vs. disfluency) was manipulated across items and participants. The between-subjects factor (Difficulty Type: Generation vs. Sans Forgetcia) was manipulated between participants. For half the participants, targets were presented in sans forgetica while the other half were presented in Arial font; for the other half of participants, targets were presented with missing letters (vowels were replaced by underscores) and the other half were intact (Arial font). After a short 2 minute distractor task (anagram generation), they completed a cued recall test. During cued recall, particpants were presented 24 cues one at a time and asked to provide the target word. After they were thanked and debriefed. 

Spell checking was automated with the hunspell package in R (Ooms, 2018) using spellCheck.R. At the next step we manually examined the output to catch incorrect suggestions and to add their own corrections.  Becasuse participants were recruited in the United States, we used the American English dictionary. A nice walkthrough on how to use the package can be found in Buchcamam, De Deyne, and Montefinese (2019). Using the package, each response was corrected for misspelings. Corrected spellings are provided in the most probable order, therefore, the first suggestion is selected as the correct answer. Answers were marked correct if they provided the exact response. In order for a response to be judged correctly, the response had to match the correct answer. 

# Results

## Scoring

Accuracy was automated with the hunspell package in R (Ooms, 2018) using spellCheck.R. At the next step we manually examined the output to catch incorrect suggestions and to add their own corrections.  Becasuse participants were recruited in the United States, we used the American English dictionary. A nice walkthrough on how to use the package can be found in Buchcamam, De Deyne, and Montefinese (2019). Using the package, each response was corrected for misspelings. Corrected spellings are provided in the most probable order, therefore, the first suggestion is selected as the correct answer. Answers were marked correct if they provided the exact response. In order for a response to be judged correctly, the response had to match the correct answer. 


In Experiment 1 we found a sizeable generation effect, which replicated past work. However, we did not find a SF effect (See figure below)


```{r}
library(qualtRics)
library(tidyverse)
library(effects)
library(here)
library(lme4)
library(ggpol)
library(knitr)
library(here)

sfgen=read_csv(here("sfgenerate_final.csv"))


full_model=glmer(acc~condition*dis + (1+ dis|ResponseID) + (1+dis+condition|target), data=sfgen, contrasts = list(dis="contr.sum", condition="contr.sum"), family="binomial", control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

ef1 <- effect("condition:dis", full_model) #take final glmer model 
summary(ef1)
x1 <- as.data.frame(ef1)

bold <- element_text(face = "bold", color = "black", size = 14) #axis bold

p<- ggplot(x1, aes(dis, fit, fill=dis))+ facet_grid(~condition)+ 
  geom_bar(stat="identity", position="dodge") + 
  geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2, position=position_dodge(width=0.9),color="red") + theme_bw(base_size=14)+labs(y="Proporition Recalled on Final Test", x="Disfluency") + 
  theme(legend.position = "none") +
  scale_fill_manual(values=c("grey", "black")) + ggplot2::coord_cartesian(ylim = c(0, 1)) + theme(axis.text=bold)

p

````


# Experiment 2

The procedure in Experiment 1 could be argued to lack educational realsim. To test the effects of sans forgetica in a more relaistic situation, Experiment 2 presented participants a passage on ground water where some of the material was either: pre-highlighted, presented in SF, or presneted normally. This was a between-subjects manipulation. 


## Participants

Participants were 528 undergraduates who participated for partial completion of course credit. Sample size was calculated based on the samllest effect of interest (Lakens & Evers, 2014). In this case, we were interested in powering our study to detect a medium-sized effect size (*d* = .35). Therefore, assuming an alpha of .05 and  a desired power of 90%, a sample size of 170 is required to detect whether an effect size of .35 differs from zero. After excluding participants based on our preregistered exclusion critera, we were left with unequal group sizes. Becasue of this, we decided to run six more pariticpants per group, giving us 176 participants in each of the three conditions.

## Materials

The preregistatiron (aspredicted.org) for Experiment 2 can be found here. All materials, data, and analysis scirpts can be found here (https://osf.io/d2vy8/). The results contained herein are  computationally reproducible by going to the primary author's github and clicking on the binder button (https://github.com/jgeller112/SF_Expt1; https://github.com/jgeller112/SF_Expt2).
 
Participants read a passage  on ground water (856 words) taken from from the U.S. Geological Survey (see Yue et al.)  Eleven critical phrases ^[orginally we had 12 critical phrases but a pilot test showed that one of the questions was repeated twice so we removed one of them and also added a manipulation check question to sure participants were paying attention] each containing a different keyword, were selected from the passage (e.g., the term *recharge* was the keyword in the phrase: Water seeping down from the land surface adds to the ground water and is called recharge water.) and were either presented in SF, highlighted, or unchanged. Then, 11 fill-in-the blank questions were created from these phrases by deleting the keyword and asking participants to provide it on the final test (e.g., Water seeping down from the land surface adds to the ground water and is called __________ water). 

## Design and Procedure

Participants were randomly assigned to either the pre-highlighted, sans forgetica, or normal conditions. Our design employed three between-subject variables: pre-highlighting, sans forgetica, and normal.

Participants completed the experiment on-line via the qualtrics survey platform. Participant read the passage on ground water in its entirety, which they were given 6 min to do. Participants in the pre-highlighted  condition received some passages in yellow highlighting. In the sans forgetica condition, participants were presented some sentences in sans forgetcia. In the normal passage condition, participants were given sentences with no changes. All particiapnts were instructed to read the passage as though they were studying material for a class.

After the alloted time, all participants were given a brief questionnaire (2 questions) asking them to indicate their metacognitive beliefs afte reading the passage. The two questions were: “Do you feel that the presentation fo the material helped you remember" and "How likely is it that you will be able to recall material from the passage you just read on a scale of 0 (not likely to recall) to 100 (likely to recall) in 5 minutes?" Participants were then given a short distractor task (anagrams) for 3 minutes. Finally, all participants were given the 12 question fill-in-the-blank test. 



# Results

Accuracy on the fill-in-the-blank test was examined using a logistical mixed model (logit link) using the lme4 package in R (Bates, Machler, Bolker, and Walker, 2015) with passage type as a fixed effect and random intercepts for subjects and questions as random effects: acc=glmer(auto_acc~passage_type+(1|ResponseId) + (1|Question), data=data, family="binomial") Passage type was treatment coded thus estimates represent simple effects. 

We hypothesized that pre-highlighted and sans forgetica sentences would be better remembered than normal sentences and that there would be no recall differences between the highlighted and san forgetia sentences. Our hypotheses were partially supported.  Results indicated that pre-highlighted sentences were better remembered than sentences presented normally, *Estimate* = .381, *exp(B)* = 1.46, *SE* = .167, *z* = 2.281, *p* = .023 *d* = .81 [^3: odds ratios were converted to d by dividing the OR by 1.91 (Chinn, 2000)] and were marginally better remmebered than sentences presented in sans forgetcia,*Estimate* = -.317, *exp(B)* = 1.37, *SE* = .168, *z* = -1.89, *p* = .059, *d*  = .76. Critically, there was no difference between sentences presented normally and in sans forgetcia, *Estimate* = .065, *exp(B)* = 1.07, *SE* = .167, *z* = 0.386, *p* = 0.700, *d* =.04. A Bayes factor using the brms package (Burkner, 2015) was computed for the point null (0) and found that probability of this effect being zero was 15.2 to 1. 

Our hypotheses were partially supported. We hypothesized that pre-highlighted and sans forgetica sentences would be better remembered than normal sentences and that there would be no recall differences between the highlighted and san forgetia sentences. We found that pre-highlighted sentneces were better remembered than sentences presented normally and in sans forgetica. 



```{r, message=FALSE, echo=FALSE, fig.align="center", fig.height=4, fig.width=8, fig.cap=paste("Accuracy on Cued Recall Test. Error bars are 95% CI dervied from the GLMER model.")}
library(qualtRics)
library(tidyverse)
library(afex)
library(emmeans)
library(here)
library(ggpol)
library(knitr)

here()
ground <- qualtRics::read_survey(here("memory_acc_gw_final.csv"))

#data was collected until the last day of the fall semester 2019 Decemeber13th. 
# loading needed libraries
full_model=glmer(auto_acc~FL_149_DO+(1|ResponseId) + (1|Question), data=ground, family="binomial")
#fit full model

ef1 <- effect("FL_149_DO", full_model) #take final glmer model 
summary(ef1)
x1 <- as.data.frame(ef1)

bold <- element_text(face = "bold", color = "black", size = 14) #axis bold
p<- ggplot(x1, aes(FL_149_DO, fit, fill=FL_149_DO))+ 
  geom_bar(stat="identity", position="dodge") + 
  geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2, position=position_dodge(width=0.9),color="red") + theme_bw(base_size=14)+labs(y="Recall Accuracy", x="Passage Type") + 
  scale_fill_manual(values=c("grey", "black", "yellow"))+
  theme(axis.text=bold, legend.position = "none") + ggplot2::coord_cartesian(ylim = c(0, 1)) +

p 

```

# Exploratory

In Experiment 2 we also asked students about their metacognitive awarness. Specifically we asked them: "How likely is it that you will be able to recall material from the passage you just read on a scale of 0 (not likely to recall) to 100 (likely to recall) in 5 minutes?" Initials analyses suggest that the normal passage was given higher JOLs (*M* = 57.4, *SE* = 1.97) than the pre-highlighted passage (*M* = 50.3, *SE* = 1.97), t(525) = -7.08,  *p* = .023. There were no reliable differences between the pre-highlighted passage and Sans Forgetica (*M* = 53.8, *SE* = 1.97), *t*(525) = -3.52,  *p* = .415 or between the passage in Sans Forgetica and the passage presneted normally, *t*(525) = 3.56,  *p* = .406. 

One potential reason for pre-highlighted information recieving lower JOLs than the normal passage is that pre-highlighted information served to focus participants attention specific parts of the passage. Given the question, pariticpants might thought this would hinder them if tested over the passage as a whole. Future research should


```{r, fig.align="center", messages=FALSE}

jols=ground %>% 
  group_by(ResponseId, FL_149_DO)%>%
  summarise(jols=mean(Q163_1)) %>%
  ungroup() %>%
  dplyr::rename(Passage="FL_149_DO") %>% 
  dplyr::mutate(Passagetype=ifelse(Passage=="Passage", "SF", Passage))



a1 <- aov_ez("ResponseId", "jols", jols, 
             between = c("Passage")) # one way

#plot the results


ls1 <- emmeans(a1, specs = "Passage") # get the simple effects test for signifcant interaction. 

flex1=pairs(ls1)

kable(flex1)

source("https://gist.githubusercontent.com/benmarwick/2a1bb0133ff568cbe28d/raw/fb53bd97121f7f9ce947837ef1a4c65a73bffb3f/geom_flat_violin.R")

raincloud_theme = theme(
text = element_text(size = 10),
axis.title.x = element_text(size = 16),
axis.title.y = element_text(size = 16),
axis.text = element_text(size = 14),
axis.text.x = element_text(angle = 45, vjust = 0.5),
legend.title=element_text(size=16),
legend.text=element_text(size=16),
legend.position = "right",
plot.title = element_text(lineheight=.8, face="bold", size = 16),
panel.border = element_blank(),
panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
axis.line.x = element_line(colour = 'black', size=0.5, linetype='solid'),
axis.line.y = element_line(colour = 'black', size=0.5, linetype='solid'))



g <- 
  ggplot(data = jols, 
         aes(x = Passage, y = jols, fill = Passage)) +
  geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .8) +
  geom_point(aes(y = jols, color = Passage), 
             position = position_jitter(width = .15), size = .5, alpha = 0.8) +
  geom_boxplot(width = .1, outlier.shape = NA, alpha = 0.5) +
  expand_limits(x = 4.00) +
  guides(fill = FALSE) +
  guides(color = FALSE) +
  scale_color_brewer(palette = "Spectral") +
  scale_fill_brewer(palette = "Spectral") +
  coord_flip() + # flip or not
  theme_bw() + labs(x="Passage Type", y="Judgements of Learning") + ggtitle("JOLs Across Manipualtion Type")
  raincloud_theme

g

#fit full model

```

We hypothezied that sentences pre-highlighted or presented in sans forgetica would be better remembered than sentences presented normally. Further, we predicted that there would be no recall differences between the pre-highligted and the sans forgetica conditions. Our hypothese were only partially confirmed. We found that infromation that was pre-hightlighted had better recall than passages presentened normally, *Estimate* = -.328, *SE* = .166, *z* = -1.97, *p* = .048. Sentences that were pre-highlighted were also remembered marginally better than senetnces presented in sans forgetica,*Estimate* = -.307, *SE* = .167, *z* = -1.84, *p* = .066. Looking at Bayes Factor for this comparison suggests that evidence for a difference between the two conditions is faily weak. Critically, sentences presented in sans forgetcia were not better remembered than sentences presented normally, *Estimate* = -.328, *SE* = .166, *z* = -1.97, *p* = .048, *BF*=).  


# Dicussion


Across two experiment 
The evidence contained herein suggests that SF does not have the mnemonic effects pruported by its creators. Now it is possible that there is an effect of SF, but the effect size might be smaller than we could detect acorss our two studies. Our SESOI was d = .35. If so, it probably does not have any real educational benefit. It is our conclsuion that SF is really forgetable and you should not be using it as a way to boost leanring. 


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
